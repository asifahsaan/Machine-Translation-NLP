{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifahsaan/Machine-Translation-NLP/blob/master/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxZCvGswmCbk"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import helper\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OSnkMqdGyuWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"eng_-french.csv\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KdsjusZfy0I3",
        "outputId": "a0aafd22-2124-4e6e-adde-70a9e90b698c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  English words/sentences  \\\n",
              "0                                                     Hi.   \n",
              "1                                                    Run!   \n",
              "2                                                    Run!   \n",
              "3                                                    Who?   \n",
              "4                                                    Wow!   \n",
              "...                                                   ...   \n",
              "175616  Top-down economics never works, said Obama. \"T...   \n",
              "175617  A carbon footprint is the amount of carbon dio...   \n",
              "175618  Death is something that we're often discourage...   \n",
              "175619  Since there are usually multiple websites on a...   \n",
              "175620  If someone who doesn't know your background sa...   \n",
              "\n",
              "                                   French words/sentences  \n",
              "0                                                  Salut!  \n",
              "1                                                 Cours !  \n",
              "2                                                Courez !  \n",
              "3                                                   Qui ?  \n",
              "4                                              Ça alors !  \n",
              "...                                                   ...  \n",
              "175616  « L'économie en partant du haut vers le bas, ç...  \n",
              "175617  Une empreinte carbone est la somme de pollutio...  \n",
              "175618  La mort est une chose qu'on nous décourage sou...  \n",
              "175619  Puisqu'il y a de multiples sites web sur chaqu...  \n",
              "175620  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
              "\n",
              "[175621 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0af08393-558e-4d3b-807d-a70ebb20caea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175616</th>\n",
              "      <td>Top-down economics never works, said Obama. \"T...</td>\n",
              "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175617</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175618</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175619</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175620</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175621 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0af08393-558e-4d3b-807d-a70ebb20caea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0af08393-558e-4d3b-807d-a70ebb20caea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0af08393-558e-4d3b-807d-a70ebb20caea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['english_sentences'] = data['English words/sentences'].str.split().str.len()\n",
        "data['french_sentences'] = data['French words/sentences'].str.split().str.len()"
      ],
      "metadata": {
        "id": "13IEaDKRcFuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "Ai4N0wLRcpR6",
        "outputId": "7316c7eb-f502-413f-d7f8-e954ad22f730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  English words/sentences  \\\n",
              "0                                                     Hi.   \n",
              "1                                                    Run!   \n",
              "2                                                    Run!   \n",
              "3                                                    Who?   \n",
              "4                                                    Wow!   \n",
              "...                                                   ...   \n",
              "175616  Top-down economics never works, said Obama. \"T...   \n",
              "175617  A carbon footprint is the amount of carbon dio...   \n",
              "175618  Death is something that we're often discourage...   \n",
              "175619  Since there are usually multiple websites on a...   \n",
              "175620  If someone who doesn't know your background sa...   \n",
              "\n",
              "                                   French words/sentences  english_sentences  \\\n",
              "0                                                  Salut!                  1   \n",
              "1                                                 Cours !                  1   \n",
              "2                                                Courez !                  1   \n",
              "3                                                   Qui ?                  1   \n",
              "4                                              Ça alors !                  1   \n",
              "...                                                   ...                ...   \n",
              "175616  « L'économie en partant du haut vers le bas, ç...                 34   \n",
              "175617  Une empreinte carbone est la somme de pollutio...                 34   \n",
              "175618  La mort est une chose qu'on nous décourage sou...                 37   \n",
              "175619  Puisqu'il y a de multiples sites web sur chaqu...                 43   \n",
              "175620  Si quelqu'un qui ne connaît pas vos antécédent...                 44   \n",
              "\n",
              "        french_sentences  \n",
              "0                      1  \n",
              "1                      2  \n",
              "2                      2  \n",
              "3                      2  \n",
              "4                      3  \n",
              "...                  ...  \n",
              "175616                47  \n",
              "175617                33  \n",
              "175618                47  \n",
              "175619                49  \n",
              "175620                55  \n",
              "\n",
              "[175621 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2aa66dd-bb79-4eb3-abd6-77e9438324b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "      <th>english_sentences</th>\n",
              "      <th>french_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175616</th>\n",
              "      <td>Top-down economics never works, said Obama. \"T...</td>\n",
              "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
              "      <td>34</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175617</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "      <td>34</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175618</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "      <td>37</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175619</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175620</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "      <td>44</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175621 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2aa66dd-bb79-4eb3-abd6-77e9438324b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2aa66dd-bb79-4eb3-abd6-77e9438324b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2aa66dd-bb79-4eb3-abd6-77e9438324b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "_sEXlz1rcshx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)"
      ],
      "metadata": {
        "id": "jQabi4mFc85f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['english_sentences'].max(), data['french_sentences'].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZkGlzjEdO84",
        "outputId": "15e87cef-39b4-410a-dbc5-e02257982302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid = train_test_split(train, test_size=0.2)"
      ],
      "metadata": {
        "id": "czoUnI7ldZEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(raw_lines):\n",
        "  text=[]\n",
        "  for raw_line in raw_lines:\n",
        "    text.append('<start>' + raw_line + '<end>')\n",
        "  return text"
      ],
      "metadata": {
        "id": "_pKQQxUwdpi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train = get_data(list(train['English words/sentences']))"
      ],
      "metadata": {
        "id": "zTwN-BuHd_8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train"
      ],
      "metadata": {
        "id": "MuzAXEf4eF3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52268537-5017-4196-d3cb-911b1c240b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>Tom left in his car.<end>',\n",
              " \"<start>Tom doesn't remember turning off the light.<end>\",\n",
              " '<start>You must be homesick.<end>',\n",
              " '<start>Nobody will help you.<end>',\n",
              " '<start>I think I could get used to this.<end>',\n",
              " '<start>Mary wanted to marry a man with ambition.<end>',\n",
              " \"<start>Don't you get it? This isn't about you.<end>\",\n",
              " '<start>My mother loves me.<end>',\n",
              " '<start>I wrote a letter in English.<end>',\n",
              " '<start>Is it snowing?<end>',\n",
              " \"<start>I'm surprised.<end>\",\n",
              " '<start>She can jump high.<end>',\n",
              " \"<start>We're all vulnerable.<end>\",\n",
              " '<start>I can carry that for you.<end>',\n",
              " '<start>This river runs through my village.<end>',\n",
              " \"<start>I don't know what I'd do without it.<end>\",\n",
              " \"<start>He doesn't stand a chance against his opponent.<end>\",\n",
              " '<start>I am proud of being a doctor.<end>',\n",
              " \"<start>You're just like your father.<end>\",\n",
              " \"<start>I don't remember her name anymore.<end>\",\n",
              " '<start>Children depend on their parents for food and clothing.<end>',\n",
              " \"<start>I didn't ask you that.<end>\",\n",
              " '<start>I lied to protect you.<end>',\n",
              " \"<start>Tom won't come today.<end>\",\n",
              " '<start>I need to get something done very quickly.<end>',\n",
              " \"<start>Why didn't you just leave?<end>\",\n",
              " '<start>In autumn, leaves change color and fall.<end>',\n",
              " '<start>I like dogs very much.<end>',\n",
              " '<start>He admitted his defeat.<end>',\n",
              " \"<start>You've got to stop this.<end>\",\n",
              " \"<start>It's against my principles.<end>\",\n",
              " '<start>They found nothing.<end>',\n",
              " '<start>He has enough willpower.<end>',\n",
              " \"<start>I wanted to discuss this with you yesterday, but you didn't seem to want to listen.<end>\",\n",
              " '<start>I had difficulty getting a ticket for the concert.<end>',\n",
              " \"<start>I've been looking for them for more than one hour.<end>\",\n",
              " '<start>I saw Tom going into the cave.<end>',\n",
              " \"<start>I'm writing a letter to my daughter.<end>\",\n",
              " \"<start>I should've worn a short-sleeved shirt.<end>\",\n",
              " '<start>Friends are always willing to help each other.<end>',\n",
              " \"<start>This is what'll happen.<end>\",\n",
              " '<start>What am I supposed to do with that?<end>',\n",
              " '<start>I prefer to not talk about it.<end>',\n",
              " '<start>Cheer up!<end>',\n",
              " \"<start>No one's going to sing here tonight.<end>\",\n",
              " '<start>There are no coincidences.<end>',\n",
              " '<start>Are you ready to do that?<end>',\n",
              " '<start>Please explain the rule to me.<end>',\n",
              " '<start>You were seen.<end>',\n",
              " '<start>How old do you think she is?<end>',\n",
              " '<start>I have a dream.<end>',\n",
              " \"<start>I'm in love with you, Tom.<end>\",\n",
              " '<start>My decision to study abroad surprised my parents.<end>',\n",
              " '<start>Set Tom free.<end>',\n",
              " '<start>Tom likes them.<end>',\n",
              " \"<start>It's a beautiful day.<end>\",\n",
              " \"<start>I don't want to drink anything.<end>\",\n",
              " \"<start>I'd like to take a couple of days off next week.<end>\",\n",
              " '<start>Tom looks older than he really is.<end>',\n",
              " '<start>Tom seems to be unable to interact normally with other people.<end>',\n",
              " '<start>I had my suitcase carried up to my room.<end>',\n",
              " '<start>There is no reason for you to feel inferior to anyone.<end>',\n",
              " '<start>Tom thinks the plan may backfire.<end>',\n",
              " '<start>Please freeze the fish and meat.<end>',\n",
              " \"<start>Our project didn't get off the ground until he joined the company.<end>\",\n",
              " \"<start>You're putting words in my mouth.<end>\",\n",
              " '<start>Accidents will happen.<end>',\n",
              " '<start>In the U.S., it is illegal to torture people in order to get information from them.<end>',\n",
              " '<start>Older people often fear change.<end>',\n",
              " \"<start>Don't look at us.<end>\",\n",
              " \"<start>If my wife calls, just tell her I'm in an important meeting and cannot be disturbed.<end>\",\n",
              " '<start>Have you completely lost your mind?<end>',\n",
              " \"<start>I didn't want you to miss your bus.<end>\",\n",
              " '<start>If I knew it, I would tell you.<end>',\n",
              " '<start>Time for dinner.<end>',\n",
              " \"<start>Don't they drive you mad?<end>\",\n",
              " \"<start>I'm not at all interested in physics.<end>\",\n",
              " '<start>Tom is on campus.<end>',\n",
              " \"<start>I didn't mean to offend you.<end>\",\n",
              " '<start>These books are easier than those books.<end>',\n",
              " '<start>Tom is a fairly good plumber.<end>',\n",
              " \"<start>I know we're a little early.<end>\",\n",
              " '<start>Do you spend more time at home or at work?<end>',\n",
              " '<start>They will be safe with him.<end>',\n",
              " '<start>Among the five of us, she, without a doubt, speaks the most languages.<end>',\n",
              " '<start>Thirteen passengers were hospitalized.<end>',\n",
              " '<start>I like trains.<end>',\n",
              " \"<start>You're the teacher.<end>\",\n",
              " '<start>He was pleasantly surprised.<end>',\n",
              " \"<start>I didn't want to get married, but my parents really wanted me to, so I got married.<end>\",\n",
              " '<start>The trouble is that it costs too much.<end>',\n",
              " '<start>Do you have anything further to say?<end>',\n",
              " '<start>Thank you again for your help.<end>',\n",
              " \"<start>This coffee is so hot that I can't drink it.<end>\",\n",
              " '<start>I feel like I know you.<end>',\n",
              " '<start>What do you say we go to my house?<end>',\n",
              " \"<start>Let's end this fast.<end>\",\n",
              " '<start>Welcome to your new home.<end>',\n",
              " '<start>We all cried.<end>',\n",
              " \"<start>I don't like shellfish.<end>\",\n",
              " '<start>We have no choice.<end>',\n",
              " '<start>He has been dead for five years.<end>',\n",
              " '<start>You could be one of them.<end>',\n",
              " \"<start>You're the one who planted that tree.<end>\",\n",
              " \"<start>I don't want you to leave.<end>\",\n",
              " '<start>Leave this to me.<end>',\n",
              " '<start>Tom was badly injured in a traffic accident.<end>',\n",
              " '<start>I remember seeing him.<end>',\n",
              " '<start>Do you know Latin?<end>',\n",
              " \"<start>Tom wouldn't let Mary keep it.<end>\",\n",
              " \"<start>She can't bear the noise.<end>\",\n",
              " '<start>It was a bad movie.<end>',\n",
              " '<start>The water in this river is very clean.<end>',\n",
              " \"<start>Tom knows that won't work.<end>\",\n",
              " \"<start>Can't you see I'm busy?<end>\",\n",
              " '<start>They arrived too soon.<end>',\n",
              " '<start>Look at that bird.<end>',\n",
              " '<start>They danced awkwardly together.<end>',\n",
              " '<start>Where did you find this evidence?<end>',\n",
              " '<start>I was really very busy.<end>',\n",
              " '<start>He was so sad that he almost went mad.<end>',\n",
              " '<start>Have any of your friends ever been arrested?<end>',\n",
              " \"<start>Are you sure you're not tired?<end>\",\n",
              " \"<start>We're deluding ourselves.<end>\",\n",
              " '<start>Thank you for all of your work.<end>',\n",
              " '<start>We would have helped them.<end>',\n",
              " '<start>Your problems are similar to mine.<end>',\n",
              " '<start>I only wish to help you.<end>',\n",
              " \"<start>I don't forget my friends.<end>\",\n",
              " '<start>I want a status report.<end>',\n",
              " '<start>Are we all going to die?<end>',\n",
              " \"<start>It's not all bad.<end>\",\n",
              " '<start>I might stay.<end>',\n",
              " '<start>He gave his blood to help his sister.<end>',\n",
              " '<start>He lost sight of the bird.<end>',\n",
              " '<start>I have no one else to turn to but you.<end>',\n",
              " \"<start>I don't like oatmeal cookies.<end>\",\n",
              " '<start>Tom decided to tell Mary the truth about his illness.<end>',\n",
              " '<start>They just want to have fun.<end>',\n",
              " '<start>How did you learn to do that?<end>',\n",
              " '<start>This city is 1,600 meters above sea level.<end>',\n",
              " '<start>I need to study.<end>',\n",
              " \"<start>I thought if I broke up with you, I'd never have to see you again.<end>\",\n",
              " '<start>English is spoken in Australia.<end>',\n",
              " '<start>My job will only last two years at most.<end>',\n",
              " '<start>Where can I buy a ticket?<end>',\n",
              " \"<start>Some people don't eat meat.<end>\",\n",
              " '<start>It is high time you spilled the beans.<end>',\n",
              " \"<start>We're stunned.<end>\",\n",
              " \"<start>I'll go get the proper forms.<end>\",\n",
              " \"<start>I've decided to go with you.<end>\",\n",
              " '<start>Would you say it once more?<end>',\n",
              " '<start>At last, we got to the lake.<end>',\n",
              " '<start>I liked it.<end>',\n",
              " \"<start>What's your theory on what happened?<end>\",\n",
              " '<start>These cameras are made in Japan.<end>',\n",
              " \"<start>I'd like to talk to you about what happened.<end>\",\n",
              " '<start>The prosperity of a country depends upon its citizens.<end>',\n",
              " '<start>She protested weakly, but ended up joining in with everyone else.<end>',\n",
              " '<start>I heard that song so many times on the radio that I grew to hate it.<end>',\n",
              " '<start>The dog must stay outside!<end>',\n",
              " '<start>The ice was thick enough for me to walk on.<end>',\n",
              " \"<start>We're doing what we want to do.<end>\",\n",
              " \"<start>They won't help us.<end>\",\n",
              " '<start>I owe you something.<end>',\n",
              " \"<start>It's all nonsense.<end>\",\n",
              " \"<start>I've been doing this my whole life.<end>\",\n",
              " '<start>Nail the windows shut.<end>',\n",
              " \"<start>Where's the nearest drugstore?<end>\",\n",
              " '<start>All I want is for you to be happy.<end>',\n",
              " \"<start>No matter how hard you try, you won't be able to finish that in one day.<end>\",\n",
              " \"<start>Don't use slang if you can help it.<end>\",\n",
              " \"<start>You're so weird.<end>\",\n",
              " \"<start>I don't like my job.<end>\",\n",
              " '<start>Please help yourself to the cake.<end>',\n",
              " '<start>I think they want you to do it.<end>',\n",
              " \"<start>I don't have time to say this twice, so listen carefully.<end>\",\n",
              " \"<start>I'm looking for a gift for a friend of mine.<end>\",\n",
              " \"<start>What you're doing makes me nervous.<end>\",\n",
              " '<start>Who would care?<end>',\n",
              " '<start>There had never been any ill-feeling between them until that night.<end>',\n",
              " \"<start>I don't like this type of house.<end>\",\n",
              " \"<start>Let's not exaggerate the facts.<end>\",\n",
              " \"<start>I finally have enough money to buy the kind of car I've always wanted.<end>\",\n",
              " \"<start>Your sister's sleeping.<end>\",\n",
              " '<start>She was promoted.<end>',\n",
              " '<start>Scorpions are dangerous.<end>',\n",
              " \"<start>Come on! I can't wait any more.<end>\",\n",
              " '<start>We were just playing.<end>',\n",
              " \"<start>Tom didn't answer because he didn't know what to say.<end>\",\n",
              " \"<start>I have a bottle of very good wine that I've been saving for a special occasion.<end>\",\n",
              " \"<start>Let's go swimming with Tom.<end>\",\n",
              " '<start>Never give up on your dreams.<end>',\n",
              " '<start>Can you help look after the kids?<end>',\n",
              " \"<start>I'm glad to be here.<end>\",\n",
              " '<start>See you tomorrow.<end>',\n",
              " '<start>She stayed at the hotel for several days.<end>',\n",
              " '<start>Tom is getting freaked out.<end>',\n",
              " \"<start>I'd like you to make one now.<end>\",\n",
              " \"<start>We've been married for 30 years.<end>\",\n",
              " '<start>Do you know why they stopped talking?<end>',\n",
              " '<start>I think I can do it.<end>',\n",
              " '<start>This knife cuts well.<end>',\n",
              " \"<start>I'll go with you.<end>\",\n",
              " \"<start>The house is small, but it's enough for us.<end>\",\n",
              " '<start>How old were you when you learned to write your name?<end>',\n",
              " '<start>She earns more than she spends.<end>',\n",
              " \"<start>I understand now why he didn't go to a university.<end>\",\n",
              " '<start>I feel flattered.<end>',\n",
              " '<start>He struck a match, but quickly put it out.<end>',\n",
              " '<start>Open up the package.<end>',\n",
              " '<start>Tom turned over a new leaf when he met Mary.<end>',\n",
              " '<start>A home is more than a mere building.<end>',\n",
              " '<start>Do you really want to buy a car now?<end>',\n",
              " \"<start>I don't know Tom well enough to know whether I like him or not.<end>\",\n",
              " \"<start>I'm working.<end>\",\n",
              " \"<start>I know what it's like to be heartbroken.<end>\",\n",
              " \"<start>You'll be perfect for this.<end>\",\n",
              " '<start>I really like my coworkers.<end>',\n",
              " \"<start>I don't feel very happy.<end>\",\n",
              " \"<start>Dogs aren't allowed in my apartment building.<end>\",\n",
              " \"<start>How did you know where I'd be?<end>\",\n",
              " '<start>I could fix that easily.<end>',\n",
              " '<start>I think we should follow Tom.<end>',\n",
              " '<start>Who am I to complain?<end>',\n",
              " \"<start>I'm afraid I've offended you.<end>\",\n",
              " '<start>Do you want wine?<end>',\n",
              " \"<start>Tom wasn't wearing his seat belt.<end>\",\n",
              " \"<start>I'll get you out of this horrible situation.<end>\",\n",
              " '<start>At first, I mistook you for your brother.<end>',\n",
              " '<start>Wait for us.<end>',\n",
              " \"<start>I never thought I'd be doing this alone.<end>\",\n",
              " \"<start>It's something they always wanted to do.<end>\",\n",
              " '<start>A man was complaining of something in a sharp voice.<end>',\n",
              " '<start>She shrieked.<end>',\n",
              " \"<start>Tom's house is very big.<end>\",\n",
              " \"<start>I can't complain about the way I've been treated.<end>\",\n",
              " \"<start>I think it's time for me to talk to the boss about this problem.<end>\",\n",
              " '<start>Tom clearly lied.<end>',\n",
              " \"<start>You don't have to stay in the hospital.<end>\",\n",
              " '<start>The floor was covered with blood.<end>',\n",
              " '<start>I hear something.<end>',\n",
              " '<start>Which credit cards can I use?<end>',\n",
              " '<start>This will make you stronger.<end>',\n",
              " \"<start>I don't have anything to read.<end>\",\n",
              " '<start>I wanted to hit him, but he ran away from me.<end>',\n",
              " '<start>They found it.<end>',\n",
              " '<start>The plane left after a three-hour delay.<end>',\n",
              " '<start>Who cheated?<end>',\n",
              " '<start>Cats are cute.<end>',\n",
              " '<start>I had two copies of the book.<end>',\n",
              " \"<start>I'm not buying you another drink until you say sorry.<end>\",\n",
              " '<start>It may rain around noon.<end>',\n",
              " \"<start>I've got this one covered.<end>\",\n",
              " '<start>I think Tom came here to tell us something important.<end>',\n",
              " '<start>This room is not very large.<end>',\n",
              " '<start>What exactly happened?<end>',\n",
              " \"<start>I hate being alone on Valentine's Day.<end>\",\n",
              " \"<start>I'm thinking of visiting you one of these days.<end>\",\n",
              " '<start>Turn right at the next intersection.<end>',\n",
              " '<start>He got a better score than us.<end>',\n",
              " '<start>She looked after her baby.<end>',\n",
              " \"<start>Don't leave me.<end>\",\n",
              " \"<start>I don't know what else we can do.<end>\",\n",
              " '<start>Could you change my room to one with a view of the ocean?<end>',\n",
              " \"<start>Don't touch my car.<end>\",\n",
              " '<start>How old is the oldest person you know?<end>',\n",
              " '<start>She said that she would follow him no matter where he went.<end>',\n",
              " '<start>Your mother died yesterday.<end>',\n",
              " '<start>My father has gone to America.<end>',\n",
              " \"<start>I left home early so I wouldn't miss the train.<end>\",\n",
              " '<start>Maybe he likes you, too.<end>',\n",
              " \"<start>You're really awesome.<end>\",\n",
              " \"<start>Who're you looking for?<end>\",\n",
              " \"<start>That's exactly what I've needed.<end>\",\n",
              " '<start>I got busy.<end>',\n",
              " '<start>These politicians are corrupt.<end>',\n",
              " \"<start>I've hurt myself.<end>\",\n",
              " \"<start>I don't know what motivated me to come here.<end>\",\n",
              " '<start>He was educated by her grandfather.<end>',\n",
              " '<start>It is not easy to be understood by everybody.<end>',\n",
              " '<start>You can not miss it.<end>',\n",
              " \"<start>Why can't I go?<end>\",\n",
              " \"<start>You'd do the same thing if you were me.<end>\",\n",
              " '<start>There was a cottage on the side of the hill.<end>',\n",
              " \"<start>Some people think that gambling's a sin.<end>\",\n",
              " \"<start>Aren't you annoyed by that?<end>\",\n",
              " \"<start>We're enemies.<end>\",\n",
              " '<start>He complained to her about the food.<end>',\n",
              " '<start>He told us to depart at once.<end>',\n",
              " \"<start>We'll have a better chance of surviving if we stay calm.<end>\",\n",
              " '<start>The door opened suddenly.<end>',\n",
              " \"<start>It's clean.<end>\",\n",
              " '<start>What do you want now?<end>',\n",
              " '<start>Do you have any plans for today?<end>',\n",
              " \"<start>I'm convinced that you're right.<end>\",\n",
              " '<start>English is like a universal language.<end>',\n",
              " \"<start>You aren't as short as I am.<end>\",\n",
              " '<start>Nothing would make me happier than to see you happy.<end>',\n",
              " '<start>He hesitated for a while.<end>',\n",
              " '<start>Nobody can be that lucky.<end>',\n",
              " \"<start>I can't see.<end>\",\n",
              " \"<start>You'll get there in time, as long as you don't miss the train.<end>\",\n",
              " '<start>At last, the baby fell asleep.<end>',\n",
              " \"<start>I didn't realize you were awake.<end>\",\n",
              " '<start>You might be interested in this.<end>',\n",
              " '<start>I like this song.<end>',\n",
              " '<start>You should be used to this by now.<end>',\n",
              " '<start>I took a taxi from the station to the hotel.<end>',\n",
              " '<start>My family will be away for a week.<end>',\n",
              " '<start>Let us stop to think how much we depend upon atomic energy.<end>',\n",
              " \"<start>I know that you aren't stupid.<end>\",\n",
              " \"<start>I don't have Tom's phone number.<end>\",\n",
              " \"<start>The telephone was just ringing, wasn't it?<end>\",\n",
              " '<start>Tom married the most beautiful girl in the city.<end>',\n",
              " \"<start>We're all retired.<end>\",\n",
              " \"<start>We're still friends.<end>\",\n",
              " '<start>She accused me of telling a lie.<end>',\n",
              " '<start>I want to live in a small town.<end>',\n",
              " '<start>Is something bothering you?<end>',\n",
              " '<start>The circumstances were different then.<end>',\n",
              " \"<start>It's obvious you're wrong.<end>\",\n",
              " '<start>He is a professor of English at Leeds.<end>',\n",
              " '<start>The storm had a serious effect on the economy.<end>',\n",
              " '<start>I thought Tom was one of them.<end>',\n",
              " '<start>I took a taxi because the bus was late.<end>',\n",
              " '<start>I need to know your answer by Friday.<end>',\n",
              " \"<start>This broken vase can't be repaired.<end>\",\n",
              " \"<start>I thought they wouldn't come.<end>\",\n",
              " \"<start>I'm pretty sure I'm going to need some help.<end>\",\n",
              " '<start>This game is so hard.<end>',\n",
              " '<start>My brother is an idiot.<end>',\n",
              " '<start>According to TV news, there was a plane crash in India.<end>',\n",
              " '<start>What would you like?<end>',\n",
              " '<start>I wish you good luck.<end>',\n",
              " \"<start>That's all you need to know.<end>\",\n",
              " \"<start>Who's that cute guy I saw you with yesterday?<end>\",\n",
              " \"<start>It's about time.<end>\",\n",
              " '<start>They spent the afternoon around the pool.<end>',\n",
              " \"<start>I'm not going.<end>\",\n",
              " '<start>No one will miss me.<end>',\n",
              " '<start>Why are you angry? \"I\\'m not angry!\"<end>',\n",
              " '<start>It looks like you did a pretty good job.<end>',\n",
              " '<start>Tom was very aware that every eye in the room was on him.<end>',\n",
              " \"<start>I'm hungover.<end>\",\n",
              " '<start>Tom wore a straw hat.<end>',\n",
              " \"<start>I'm going to church.<end>\",\n",
              " '<start>You have a future.<end>',\n",
              " \"<start>The fire started in Tom's room.<end>\",\n",
              " '<start>She choked him.<end>',\n",
              " '<start>Is this your first investigation?<end>',\n",
              " '<start>She wore a simple dress.<end>',\n",
              " '<start>Dad gave me a computer game.<end>',\n",
              " \"<start>One must keep one's promises.<end>\",\n",
              " \"<start>Don't read in this room.<end>\",\n",
              " '<start>Do you like to sing?<end>',\n",
              " \"<start>You're very funny.<end>\",\n",
              " '<start>Will I receive any help?<end>',\n",
              " '<start>We have a relationship.<end>',\n",
              " '<start>Tom is someone I really respect.<end>',\n",
              " \"<start>She's my type.<end>\",\n",
              " \"<start>Tom said he didn't want to eat anything.<end>\",\n",
              " \"<start>We've got to get you to a hospital.<end>\",\n",
              " \"<start>I really don't have enough money.<end>\",\n",
              " '<start>Could you get it for me now?<end>',\n",
              " '<start>I really need to talk with someone.<end>',\n",
              " \"<start>That accident is a very good example of what happens when you're not careful.<end>\",\n",
              " '<start>You came at just the right time.<end>',\n",
              " \"<start>Don't interrupt me.<end>\",\n",
              " '<start>Give it to anyone you like.<end>',\n",
              " '<start>Do you have any idea what happened?<end>',\n",
              " \"<start>You like cats, don't you?<end>\",\n",
              " '<start>The company accepted his application.<end>',\n",
              " \"<start>Mary isn't Tom's wife.<end>\",\n",
              " \"<start>I don't know about that.<end>\",\n",
              " '<start>Who wants to go hunting?<end>',\n",
              " \"<start>I thought you'd left.<end>\",\n",
              " '<start>She wrapped the present in paper.<end>',\n",
              " \"<start>You don't seem convinced.<end>\",\n",
              " '<start>Please refer to page ten.<end>',\n",
              " \"<start>I'm almost always at home on Mondays.<end>\",\n",
              " '<start>I like having plenty to do.<end>',\n",
              " '<start>You need to sleep.<end>',\n",
              " '<start>My camera is broken.<end>',\n",
              " '<start>I finally gave up smoking.<end>',\n",
              " '<start>Tom is a geologist.<end>',\n",
              " '<start>She has a little bread.<end>',\n",
              " '<start>You can handle this without me.<end>',\n",
              " '<start>Tom kept Mary waiting for an hour.<end>',\n",
              " '<start>Has Flight 123 arrived?<end>',\n",
              " \"<start>That's what got me into this line of work.<end>\",\n",
              " '<start>This laptop computer is very thin.<end>',\n",
              " '<start>Tom gave Mary a French dictionary.<end>',\n",
              " \"<start>I'm in the house.<end>\",\n",
              " '<start>Go and get a chair from the next room, please.<end>',\n",
              " \"<start>Tom knew that he'd been tricked.<end>\",\n",
              " '<start>We have less than three minutes left.<end>',\n",
              " '<start>Italy is a peninsula.<end>',\n",
              " '<start>Can I have my bicycle back?<end>',\n",
              " '<start>There are a great many forest fires in America.<end>',\n",
              " '<start>She felt her heart beat quickly.<end>',\n",
              " '<start>You look uglier than usual.<end>',\n",
              " \"<start>I don't like the new guy.<end>\",\n",
              " '<start>You sure do smoke a lot.<end>',\n",
              " \"<start>I don't sleep anymore.<end>\",\n",
              " '<start>Sweep my room.<end>',\n",
              " '<start>My TV set is almost 15 years old, but it still has a good picture.<end>',\n",
              " \"<start>It doesn't matter if you do that or not.<end>\",\n",
              " \"<start>It's driving me crazy.<end>\",\n",
              " \"<start>We're brothers.<end>\",\n",
              " '<start>I forgot my glasses somewhere.<end>',\n",
              " \"<start>Where's your watch?<end>\",\n",
              " '<start>What is in the garden?<end>',\n",
              " '<start>He seems to be ill.<end>',\n",
              " '<start>I studied for one hour.<end>',\n",
              " '<start>You are too young to travel alone.<end>',\n",
              " '<start>He goes to China in May.<end>',\n",
              " '<start>You need to help me.<end>',\n",
              " '<start>The building collapsed in the earthquake.<end>',\n",
              " '<start>Tell me where you went.<end>',\n",
              " '<start>Are you racist?<end>',\n",
              " '<start>We promised to stand by him in case of trouble.<end>',\n",
              " \"<start>You're creative.<end>\",\n",
              " \"<start>He's waiting for you at home.<end>\",\n",
              " '<start>I made muffins.<end>',\n",
              " '<start>I feel sorry for her.<end>',\n",
              " \"<start>I'm not a murderer.<end>\",\n",
              " '<start>Can you understand Tom?<end>',\n",
              " '<start>I have a granddaughter about your age.<end>',\n",
              " '<start>Was it funny?<end>',\n",
              " '<start>Send for the doctor at once.<end>',\n",
              " '<start>I thought I knew all your secrets.<end>',\n",
              " '<start>I thought you were in charge.<end>',\n",
              " '<start>What would I do without you?<end>',\n",
              " \"<start>I'm having the time of my life.<end>\",\n",
              " '<start>She told me that I could sleep on the sofa.<end>',\n",
              " '<start>I never imagined so many people would come to my party.<end>',\n",
              " \"<start>I didn't feel well.<end>\",\n",
              " \"<start>I'm leaving tonight.<end>\",\n",
              " '<start>Now means now.<end>',\n",
              " '<start>What time is the concert?<end>',\n",
              " '<start>Do you want to become a father?<end>',\n",
              " '<start>This is the best method.<end>',\n",
              " '<start>I had an amazing experience.<end>',\n",
              " '<start>I know where they will be this afternoon.<end>',\n",
              " \"<start>As you can see, I haven't done any cleaning in the house for some time.<end>\",\n",
              " '<start>No one told me.<end>',\n",
              " \"<start>You're elusive.<end>\",\n",
              " '<start>Can you introduce me to a lawyer who speaks French?<end>',\n",
              " \"<start>We weren't all that busy.<end>\",\n",
              " \"<start>This coat doesn't fit me.<end>\",\n",
              " '<start>Some children do not like vegetables.<end>',\n",
              " \"<start>We're going east.<end>\",\n",
              " '<start>Helen Keller was deaf and blind.<end>',\n",
              " \"<start>I'm the one who's sick.<end>\",\n",
              " '<start>He wants to reach a wider audience.<end>',\n",
              " '<start>Tom could face life in prison.<end>',\n",
              " '<start>She just left.<end>',\n",
              " '<start>He removed his sunglasses.<end>',\n",
              " \"<start>You don't need to explain that.<end>\",\n",
              " \"<start>Don't scare me like that, OK?<end>\",\n",
              " '<start>I just got back from my morning swim.<end>',\n",
              " \"<start>I should've done that somewhere else.<end>\",\n",
              " '<start>It has been many years since she died.<end>',\n",
              " '<start>We all missed the target.<end>',\n",
              " \"<start>I'm not married.<end>\",\n",
              " '<start>Are they satisfied?<end>',\n",
              " '<start>I saw a big pelican there.<end>',\n",
              " '<start>Tom got out of the bathtub and dried himself with the new towel that Mary had given him.<end>',\n",
              " '<start>She advised him to see a lawyer, so he did.<end>',\n",
              " \"<start>I was lucky that the train was late. Otherwise I would've missed it.<end>\",\n",
              " '<start>We ate fresh fruit after dinner.<end>',\n",
              " '<start>Everyone wants to be young and attractive.<end>',\n",
              " \"<start>I'm used to this sort of thing.<end>\",\n",
              " '<start>I never wanted that.<end>',\n",
              " '<start>What does this mean?<end>',\n",
              " '<start>A little heavier rain might cause a flood.<end>',\n",
              " '<start>Hurry up, and you will be able to catch the train.<end>',\n",
              " \"<start>I never thought I'd ever get married.<end>\",\n",
              " \"<start>You've got a big mouth.<end>\",\n",
              " '<start>The sun was shining, yet it was cold.<end>',\n",
              " '<start>I can give you something for your pain.<end>',\n",
              " \"<start>It's a no-brainer.<end>\",\n",
              " '<start>All of you speak French, right?<end>',\n",
              " '<start>Are we allowed to use the elevator?<end>',\n",
              " \"<start>She doesn't live with him.<end>\",\n",
              " \"<start>I'm pleased with my new bathing suit.<end>\",\n",
              " '<start>I want to talk to you about this list.<end>',\n",
              " '<start>I would like a large slice of cake and a cup of coffee.<end>',\n",
              " '<start>They were enjoying themselves.<end>',\n",
              " '<start>I have a lot of friends that can speak French well.<end>',\n",
              " \"<start>I don't tell people what to eat.<end>\",\n",
              " '<start>He arrived at the station out of breath.<end>',\n",
              " '<start>Who invited you here?<end>',\n",
              " \"<start>The baby doesn't walk yet.<end>\",\n",
              " '<start>How long have you been out of prison?<end>',\n",
              " '<start>Your daughter overheard us talking.<end>',\n",
              " '<start>I want you to have my land after I die.<end>',\n",
              " '<start>She agreed with him that I should go to the meeting.<end>',\n",
              " '<start>Help us.<end>',\n",
              " '<start>Do you have the same thing in a different color?<end>',\n",
              " '<start>Better a little than nothing.<end>',\n",
              " '<start>It was the only thing to do.<end>',\n",
              " \"<start>I'm so proud of you.<end>\",\n",
              " '<start>They will demolish the building and replace it with a park.<end>',\n",
              " '<start>I only got your letter yesterday.<end>',\n",
              " '<start>He has an uncontrollable temper.<end>',\n",
              " '<start>I did that all alone.<end>',\n",
              " \"<start>I'm not good at classifying things.<end>\",\n",
              " '<start>What is it you want me to do?<end>',\n",
              " '<start>I’ll call him before he goes out.<end>',\n",
              " '<start>Where do you think I met her?<end>',\n",
              " \"<start>It's this book.<end>\",\n",
              " \"<start>Tom didn't understand a word Mary said.<end>\",\n",
              " '<start>It cost a lot more than I thought it would.<end>',\n",
              " '<start>Tom has a meeting this morning.<end>',\n",
              " '<start>Tom went home for the weekend.<end>',\n",
              " '<start>Tom is as tall as his father.<end>',\n",
              " '<start>If I have any more questions, where can I find you?<end>',\n",
              " '<start>The family had been sleeping for about two hours when the fire broke out.<end>',\n",
              " \"<start>I'm not sure I follow what you're saying.<end>\",\n",
              " \"<start>I think it's not going to be that hard.<end>\",\n",
              " '<start>The boy has never been to the zoo.<end>',\n",
              " '<start>My brother is good at mathematics.<end>',\n",
              " \"<start>Tom wasn't wearing his seat belt.<end>\",\n",
              " '<start>Tom tasted the wine.<end>',\n",
              " '<start>He is a good singer.<end>',\n",
              " '<start>I threw a ball to my dog and he caught it in his mouth.<end>',\n",
              " '<start>She saw him driving his new car.<end>',\n",
              " '<start>Show me your passport, please.<end>',\n",
              " '<start>Are you coming?<end>',\n",
              " '<start>This is my new video.<end>',\n",
              " \"<start>Don't breathe a word of what I've told you to anyone.<end>\",\n",
              " \"<start>That's a very interesting question.<end>\",\n",
              " '<start>How was your trip to Australia?<end>',\n",
              " '<start>She insulted him.<end>',\n",
              " '<start>He took out a dollar from his wallet.<end>',\n",
              " '<start>They will hold talks tomorrow.<end>',\n",
              " '<start>I know these students.<end>',\n",
              " '<start>Where are you going with this?<end>',\n",
              " \"<start>It's cool today.<end>\",\n",
              " \"<start>Let's go to the picnic.<end>\",\n",
              " '<start>You have no one to blame but yourself.<end>',\n",
              " '<start>He stopped the car.<end>',\n",
              " \"<start>Let's not deviate from the subject.<end>\",\n",
              " '<start>Tom and Mary are playing a video game.<end>',\n",
              " '<start>He did it in good faith.<end>',\n",
              " \"<start>Tom was at Mary's house on Monday afternoon.<end>\",\n",
              " '<start>He gave up his dream of becoming a pilot.<end>',\n",
              " '<start>Tom seems to be contributing.<end>',\n",
              " '<start>My grandmother lives in the country.<end>',\n",
              " '<start>Sit down and shut up.<end>',\n",
              " \"<start>Aren't you surprised to see me?<end>\",\n",
              " '<start>I felt my phone vibrate in my pocket.<end>',\n",
              " '<start>The hospital has three wings.<end>',\n",
              " '<start>She showed us a beautiful hat.<end>',\n",
              " '<start>I thought I heard you.<end>',\n",
              " '<start>It looks like you just saw a ghost.<end>',\n",
              " '<start>The light went on.<end>',\n",
              " \"<start>Tom has no conception of what it's like to be in love.<end>\",\n",
              " '<start>The production of vegetables is growing in our area.<end>',\n",
              " '<start>Words failed me.<end>',\n",
              " \"<start>How come you didn't come to the party?<end>\",\n",
              " '<start>He walked quietly.<end>',\n",
              " '<start>Tom is listening to music.<end>',\n",
              " \"<start>I stayed at Tom's house in Boston.<end>\",\n",
              " '<start>I sell shoes.<end>',\n",
              " '<start>Is there anything special you want to do this weekend?<end>',\n",
              " '<start>This is one of the best schools in the country.<end>',\n",
              " '<start>Are you laughing at me?<end>',\n",
              " '<start>He is a member of the parish committee.<end>',\n",
              " '<start>I am a stranger here.<end>',\n",
              " \"<start>We don't need it anymore.<end>\",\n",
              " '<start>It took me only three hours to do that.<end>',\n",
              " '<start>Walk ahead of me.<end>',\n",
              " '<start>The architect achieved worldwide fame.<end>',\n",
              " '<start>Just tell Tom to leave me alone.<end>',\n",
              " \"<start>I'm getting along with my mother-in-law very well.<end>\",\n",
              " '<start>Tom was screaming at Mary.<end>',\n",
              " '<start>I used my imagination.<end>',\n",
              " '<start>Tom wants to study music.<end>',\n",
              " '<start>Tom tried to drink his problems away.<end>',\n",
              " \"<start>I'm not afraid of heights.<end>\",\n",
              " '<start>There was a car accident yesterday.<end>',\n",
              " '<start>Are you sure you can do this?<end>',\n",
              " '<start>It was so good.<end>',\n",
              " '<start>It takes more than one swallow to make a summer.<end>',\n",
              " '<start>You need to wake up.<end>',\n",
              " \"<start>Well, you've convinced me.<end>\",\n",
              " '<start>That was completely unacceptable.<end>',\n",
              " '<start>What if someone killed Tom? What would you do?<end>',\n",
              " '<start>He seems very pleasant.<end>',\n",
              " \"<start>Excuse me, but I didn't order this.<end>\",\n",
              " \"<start>She can't do that.<end>\",\n",
              " '<start>Tom is guilty.<end>',\n",
              " \"<start>I'm already in a lot of trouble.<end>\",\n",
              " \"<start>I'll miss this place once I leave.<end>\",\n",
              " '<start>If you want to be free, destroy your television set.<end>',\n",
              " '<start>I want to give you a hug.<end>',\n",
              " '<start>His voice carries very well.<end>',\n",
              " \"<start>I think we're even.<end>\",\n",
              " '<start>She has an agreeable voice.<end>',\n",
              " '<start>They fought the measures in the courts.<end>',\n",
              " '<start>I met her in the winter of last year.<end>',\n",
              " \"<start>It didn't really hurt.<end>\",\n",
              " '<start>That seems inhumane to me.<end>',\n",
              " \"<start>Tom doesn't like the color of the walls in his bedroom.<end>\",\n",
              " '<start>How do you plan on paying for that?<end>',\n",
              " \"<start>I can't move my legs.<end>\",\n",
              " '<start>Consider it an emergency.<end>',\n",
              " \"<start>The air conditioner doesn't seem to work.<end>\",\n",
              " '<start>You should have seen him.<end>',\n",
              " \"<start>He was sick, so he couldn't attend the party.<end>\",\n",
              " '<start>Draw me a seven-pointed star.<end>',\n",
              " '<start>He came home late last night.<end>',\n",
              " '<start>When in Rome, do as the Romans do.<end>',\n",
              " \"<start>I am ashamed of my son's laziness.<end>\",\n",
              " \"<start>Who's got questions?<end>\",\n",
              " '<start>Both of them seem suspicious.<end>',\n",
              " '<start>Where are they?<end>',\n",
              " '<start>Can you guess where I am right now?<end>',\n",
              " '<start>It was just a fling.<end>',\n",
              " '<start>This a very significant discovery.<end>',\n",
              " '<start>Tomorrow is my birthday.<end>',\n",
              " '<start>Tom got up from the table and walked into the kitchen.<end>',\n",
              " '<start>Stay out of my way!<end>',\n",
              " '<start>I had dinner with a friend last night.<end>',\n",
              " '<start>No one can tell.<end>',\n",
              " '<start>What were you thinking?<end>',\n",
              " '<start>Excuse me one second.<end>',\n",
              " \"<start>I don't like it when you do that.<end>\",\n",
              " \"<start>I'm a good-for-nothing bum.<end>\",\n",
              " '<start>It was a great trip.<end>',\n",
              " \"<start>I've remarried.<end>\",\n",
              " \"<start>I'll be back in thirty minutes so I'll be in time for the concert.<end>\",\n",
              " \"<start>I'm sure you're busy.<end>\",\n",
              " '<start>Keep the money in a safe place.<end>',\n",
              " '<start>Clearly, that was a mistake.<end>',\n",
              " \"<start>I'll take you there.<end>\",\n",
              " '<start>Do you want to do this now?<end>',\n",
              " '<start>Did you buy Tom a dog?<end>',\n",
              " \"<start>I'm making a lot more money now.<end>\",\n",
              " '<start>He had an operation on his left leg.<end>',\n",
              " \"<start>I've still got a lot to learn.<end>\",\n",
              " '<start>I have nothing to do today.<end>',\n",
              " '<start>Stop flirting with me.<end>',\n",
              " '<start>This is free.<end>',\n",
              " '<start>I feel kind of sick.<end>',\n",
              " \"<start>I'm afraid that's impossible.<end>\",\n",
              " '<start>What are you trying to say?<end>',\n",
              " '<start>Beware of pickpockets.<end>',\n",
              " \"<start>You haven't made any mistakes.<end>\",\n",
              " '<start>Do it this way.<end>',\n",
              " \"<start>Tom isn't the only famous person here.<end>\",\n",
              " \"<start>I was hoping you'd be there.<end>\",\n",
              " \"<start>Let's not argue.<end>\",\n",
              " '<start>How long did you stay at the party?<end>',\n",
              " '<start>Should I be worried?<end>',\n",
              " '<start>I tried to stay inside the house all day, but I ended up going outside and sitting in the garden.<end>',\n",
              " \"<start>I'm tired of being treated like a child.<end>\",\n",
              " \"<start>Hey, what's this?<end>\",\n",
              " \"<start>You don't have to come see me.<end>\",\n",
              " '<start>The telephone rang.<end>',\n",
              " \"<start>I didn't understand what was happening.<end>\",\n",
              " '<start>That man is dangerous.<end>',\n",
              " \"<start>I'll get used to it.<end>\",\n",
              " '<start>We played many kinds of games.<end>',\n",
              " '<start>How much do you want to spend?<end>',\n",
              " \"<start>I'd prefer that you stay home tonight.<end>\",\n",
              " '<start>When did I give you that?<end>',\n",
              " '<start>She expected him to leave town.<end>',\n",
              " \"<start>They knew how much danger they'd be in.<end>\",\n",
              " \"<start>I don't care much for coffee.<end>\",\n",
              " \"<start>You're a good student.<end>\",\n",
              " \"<start>You aren't supposed to swim here.<end>\",\n",
              " '<start>This one belongs to you.<end>',\n",
              " '<start>He is not qualified for the job.<end>',\n",
              " '<start>Give me the salt, please.<end>',\n",
              " '<start>Doing it that way will take a long time.<end>',\n",
              " '<start>It would be better to stay home today.<end>',\n",
              " '<start>I think about you every day.<end>',\n",
              " '<start>Tell me again how much money you have.<end>',\n",
              " '<start>It worked right out of the box.<end>',\n",
              " '<start>He got the first prize in the contest.<end>',\n",
              " \"<start>His company didn't survive the crisis.<end>\",\n",
              " \"<start>I'm not supposed to drink.<end>\",\n",
              " '<start>Is it true that Tom won the race?<end>',\n",
              " '<start>I want to find out what caused the problem.<end>',\n",
              " \"<start>You look like you're busy.<end>\",\n",
              " \"<start>Tom doesn't love me.<end>\",\n",
              " '<start>We all agree with you.<end>',\n",
              " '<start>He was panting.<end>',\n",
              " '<start>May I give you some advice?<end>',\n",
              " \"<start>It's raining cats and dogs here.<end>\",\n",
              " \"<start>The United States has almost a fourth of the world's prison population.<end>\",\n",
              " '<start>We all deserve respect.<end>',\n",
              " '<start>To make a long story short, he married his first love.<end>',\n",
              " '<start>Do you really want to help Tom?<end>',\n",
              " '<start>I learned that a long time ago.<end>',\n",
              " '<start>Are you frightened?<end>',\n",
              " \"<start>I'm checking options.<end>\",\n",
              " '<start>You seem to be having fun.<end>',\n",
              " '<start>Stay a while and listen.<end>',\n",
              " '<start>Did you have any luck?<end>',\n",
              " '<start>This noise is annoying.<end>',\n",
              " '<start>Tom is always lying.<end>',\n",
              " '<start>Tom drank a glass of wine.<end>',\n",
              " '<start>It was a bitter pill to swallow.<end>',\n",
              " '<start>How do they seem to you?<end>',\n",
              " '<start>This is an option to consider.<end>',\n",
              " '<start>I will give it a try.<end>',\n",
              " '<start>Few people live to be one hundred years old.<end>',\n",
              " \"<start>Are you saying you don't want to be a teacher anymore?<end>\",\n",
              " \"<start>I think it's time for me to get my eyes checked.<end>\",\n",
              " '<start>Never put off to tomorrow what you can do today.<end>',\n",
              " '<start>Because my mother was ill, I could not go there.<end>',\n",
              " '<start>Tom is working hard to improve his French.<end>',\n",
              " '<start>How would you translate it?<end>',\n",
              " \"<start>What's in the file?<end>\",\n",
              " '<start>He died without having made a will.<end>',\n",
              " '<start>He committed one crime after another.<end>',\n",
              " '<start>Tom said that Mary died in her sleep.<end>',\n",
              " '<start>I want to live in a big city.<end>',\n",
              " '<start>He felt sad because he lost his father.<end>',\n",
              " \"<start>Promise me you won't do that again.<end>\",\n",
              " '<start>She fell from the tree.<end>',\n",
              " '<start>Perhaps you are right.<end>',\n",
              " \"<start>Tom doesn't know where to look.<end>\",\n",
              " \"<start>I never should've let you go home alone last night.<end>\",\n",
              " '<start>I have a surprise for you.<end>',\n",
              " '<start>Those houses are big.<end>',\n",
              " '<start>They supported his right to speak freely.<end>',\n",
              " '<start>He recommended this dictionary to me.<end>',\n",
              " '<start>I know Tom is from Boston.<end>',\n",
              " \"<start>Tom's arrived.<end>\",\n",
              " \"<start>I'd be delighted to sing for you.<end>\",\n",
              " '<start>How about going for a drive?<end>',\n",
              " \"<start>I don't work for anyone.<end>\",\n",
              " \"<start>We'll be at home all day today.<end>\",\n",
              " '<start>I thought you might feel the same way.<end>',\n",
              " \"<start>I'll never be able to trust you again.<end>\",\n",
              " '<start>Where did you get this information?<end>',\n",
              " '<start>The last time we ate dinner together, you would only eat vegetables.<end>',\n",
              " \"<start>We're quite alone.<end>\",\n",
              " '<start>The work is not finished yet.<end>',\n",
              " '<start>I really want to leave.<end>',\n",
              " '<start>He is not in.<end>',\n",
              " \"<start>I'm in luck.<end>\",\n",
              " '<start>All I want is to be alone for a few months.<end>',\n",
              " '<start>I was saving this piece of cake for you.<end>',\n",
              " '<start>This job pays pretty well.<end>',\n",
              " '<start>How do you feel about the issue?<end>',\n",
              " \"<start>Tom didn't want to do anything.<end>\",\n",
              " '<start>Tom and Mary stopped kissing.<end>',\n",
              " \"<start>Don't give up!<end>\",\n",
              " '<start>Put that book aside for me.<end>',\n",
              " \"<start>Tom can't hurt you anymore.<end>\",\n",
              " '<start>Do you know how to get to our place?<end>',\n",
              " \"<start>I don't feel like taking a walk this morning.<end>\",\n",
              " '<start>I freaked out.<end>',\n",
              " \"<start>We're prejudiced.<end>\",\n",
              " \"<start>It's very hot today.<end>\",\n",
              " '<start>Please paint the door white.<end>',\n",
              " \"<start>I can't find anything.<end>\",\n",
              " \"<start>I didn't want to be presumptuous.<end>\",\n",
              " '<start>Where did you work before you came here?<end>',\n",
              " '<start>I want you to leave me alone.<end>',\n",
              " '<start>She is old.<end>',\n",
              " '<start>I almost drowned.<end>',\n",
              " '<start>Have you read this article?<end>',\n",
              " '<start>Having failed several times, he tried to do it again.<end>',\n",
              " '<start>The chicken is cooked.<end>',\n",
              " '<start>This restaurant is badly managed.<end>',\n",
              " '<start>The man left the restaurant without paying his bill.<end>',\n",
              " \"<start>I'd like to run a big stock farm.<end>\",\n",
              " \"<start>I can't believe that actually happened.<end>\",\n",
              " \"<start>I hope we didn't wake you.<end>\",\n",
              " '<start>Feel free to comment on any point made here.<end>',\n",
              " \"<start>Please follow the nurse's directions.<end>\",\n",
              " '<start>Is there somebody you want to talk to?<end>',\n",
              " '<start>I get arrested from time to time.<end>',\n",
              " '<start>Corporations are competing to fill the vacuum.<end>',\n",
              " '<start>What did I ever do to you?<end>',\n",
              " '<start>Is Tom lazy?<end>',\n",
              " '<start>I came with my friends.<end>',\n",
              " \"<start>What's it called?<end>\",\n",
              " \"<start>That's the problem.<end>\",\n",
              " '<start>Do you have a better idea?<end>',\n",
              " '<start>I never dreamed of there being such a quiet place in this noisy city.<end>',\n",
              " '<start>I think it is a gift.<end>',\n",
              " '<start>My room is twice as large as yours.<end>',\n",
              " '<start>That never crossed my mind.<end>',\n",
              " \"<start>We don't want to lose.<end>\",\n",
              " '<start>The rain lasted for three days.<end>',\n",
              " '<start>This watch was given me by my uncle.<end>',\n",
              " '<start>I tore up all the letters that you wrote to me.<end>',\n",
              " \"<start>I've hurt your feelings, haven't I?<end>\",\n",
              " '<start>Our bus collided with a truck.<end>',\n",
              " \"<start>We're all a bit a scared.<end>\",\n",
              " '<start>I can hear you loud and clear.<end>',\n",
              " \"<start>I know where they're going.<end>\",\n",
              " '<start>I know how tiring that can be.<end>',\n",
              " '<start>Can I turn on the TV?<end>',\n",
              " \"<start>It's dishonest.<end>\",\n",
              " \"<start>Did you go to Tom's party last Saturday?<end>\",\n",
              " \"<start>I'll pass.<end>\",\n",
              " \"<start>I don't have any homework today.<end>\",\n",
              " '<start>You gotta get more organized.<end>',\n",
              " '<start>I also went there.<end>',\n",
              " '<start>This method works every time.<end>',\n",
              " '<start>I can confirm this.<end>',\n",
              " \"<start>I think it's highly unlikely that we'll be able to escape from this prison.<end>\",\n",
              " '<start>His absence gave birth to all sorts of rumors.<end>',\n",
              " '<start>He was kind enough to help me.<end>',\n",
              " '<start>Do we have to get rid of that?<end>',\n",
              " \"<start>I still can't believe I'm going to be on your team.<end>\",\n",
              " \"<start>We're fasting.<end>\",\n",
              " '<start>The salesclerk will come to help you right away.<end>',\n",
              " '<start>Let me die.<end>',\n",
              " '<start>Tom has been a good roommate.<end>',\n",
              " '<start>Drop me a line.<end>',\n",
              " '<start>This book belongs to you.<end>',\n",
              " '<start>Would you like ice?<end>',\n",
              " '<start>Do you intend to answer all my questions truthfully?<end>',\n",
              " '<start>I hear a woodpecker.<end>',\n",
              " '<start>I wanted to go to Australia.<end>',\n",
              " '<start>He will soon be past playing with toys.<end>',\n",
              " \"<start>I just don't want to get your hopes up.<end>\",\n",
              " \"<start>I'm sure I saw something moving.<end>\",\n",
              " \"<start>I've never had to fire anyone before.<end>\",\n",
              " \"<start>I wonder if there's any connection.<end>\",\n",
              " '<start>Do you want us to go with you?<end>',\n",
              " '<start>They hunted foxes.<end>',\n",
              " '<start>They were responsible for the accident.<end>',\n",
              " \"<start>Let's wrap up this work now and go out drinking.<end>\",\n",
              " \"<start>Don't be in such a hurry.<end>\",\n",
              " '<start>I grew up in a little town.<end>',\n",
              " \"<start>Why don't you admit your mistake?<end>\",\n",
              " '<start>You have to promise not to tell anyone.<end>',\n",
              " \"<start>It's still crowded.<end>\",\n",
              " '<start>I told him to be quiet.<end>',\n",
              " \"<start>Tom's house was almost completely destroyed.<end>\",\n",
              " '<start>I figured you might change your mind.<end>',\n",
              " '<start>Is this really all that important?<end>',\n",
              " '<start>I thought we could get together later.<end>',\n",
              " '<start>Can I eat in my room?<end>',\n",
              " \"<start>Don't you want to see the world?<end>\",\n",
              " '<start>You look exactly like Tom.<end>',\n",
              " '<start>I guess I need a little sleep.<end>',\n",
              " '<start>Why do you always butt in?<end>',\n",
              " \"<start>Isn't that an amazing coincidence?<end>\",\n",
              " '<start>Get my rifle.<end>',\n",
              " '<start>I knew it was a trick.<end>',\n",
              " '<start>Give me back my glasses.<end>',\n",
              " \"<start>I don't know what I owe you.<end>\",\n",
              " '<start>My mother spends a lot of money on clothes.<end>',\n",
              " \"<start>I thought we weren't going to go there.<end>\",\n",
              " \"<start>Tom's diet resulted in weight loss.<end>\",\n",
              " '<start>I have to get up early.<end>',\n",
              " '<start>He seems to be asleep.<end>',\n",
              " '<start>Tom poured milk on his cereal.<end>',\n",
              " \"<start>Thank you for all you've done for us.<end>\",\n",
              " '<start>Grab a broom and help us clean.<end>',\n",
              " '<start>We went into the red last year.<end>',\n",
              " '<start>I know Tom was disrespectful.<end>',\n",
              " '<start>I regret going there.<end>',\n",
              " \"<start>I hope you've got some proof to back up your allegations.<end>\",\n",
              " '<start>That book had a lot of pages.<end>',\n",
              " \"<start>I'm leaving tonight for Australia.<end>\",\n",
              " '<start>The unexpected always happens.<end>',\n",
              " '<start>Do you want to get drunk?<end>',\n",
              " '<start>Do you have a guilty conscience?<end>',\n",
              " \"<start>Let's get home.<end>\",\n",
              " '<start>I have just finished my work.<end>',\n",
              " \"<start>Don't talk to Tom.<end>\",\n",
              " '<start>I think Tom is selfish.<end>',\n",
              " \"<start>What's the fun in that?<end>\",\n",
              " '<start>What a big supermarket!<end>',\n",
              " '<start>May I turn off the light?<end>',\n",
              " '<start>I said that I could, not that I would.<end>',\n",
              " '<start>Tom is reading in his bedroom.<end>',\n",
              " '<start>He will get his job back at the next election.<end>',\n",
              " \"<start>It's time to go to bed. Turn off the radio.<end>\",\n",
              " '<start>It was very sensible of him to reject the bribe.<end>',\n",
              " '<start>He carved me a wooden doll.<end>',\n",
              " '<start>He did not come till noon.<end>',\n",
              " '<start>Tom knew what Mary had done.<end>',\n",
              " '<start>Where did you find that strange thing?<end>',\n",
              " \"<start>I don't like homework.<end>\",\n",
              " '<start>Could you download a file for me?<end>',\n",
              " '<start>I have no excuse.<end>',\n",
              " '<start>I despise you.<end>',\n",
              " \"<start>Tom wasn't complaining.<end>\",\n",
              " '<start>The mayor provided me with an identity card.<end>',\n",
              " '<start>Keep an eye on the child for me for a moment.<end>',\n",
              " '<start>I have a long way to go.<end>',\n",
              " \"<start>I don't take vacations very often.<end>\",\n",
              " '<start>There were no survivors.<end>',\n",
              " '<start>Is it helping?<end>',\n",
              " '<start>Why did Tom lie?<end>',\n",
              " \"<start>I have to do what's right.<end>\",\n",
              " '<start>The student raised his hand.<end>',\n",
              " '<start>Are you still unlucky?<end>',\n",
              " \"<start>As long as you're here, I'll stay.<end>\",\n",
              " '<start>You need to find another way out of this situation.<end>',\n",
              " '<start>I received my birthday present.<end>',\n",
              " '<start>To tell the truth, we got married last year.<end>',\n",
              " '<start>Where are you living now?<end>',\n",
              " \"<start>I didn't even think that was possible.<end>\",\n",
              " '<start>The planes arrived one after another.<end>',\n",
              " \"<start>You'll find a solution, I'm sure.<end>\",\n",
              " '<start>He ignored me.<end>',\n",
              " '<start>Maybe we can sit together.<end>',\n",
              " '<start>Wait for a few seconds.<end>',\n",
              " \"<start>I've heard that many times.<end>\",\n",
              " \"<start>We are anxious about our daughter's health.<end>\",\n",
              " \"<start>There's a possibility of war.<end>\",\n",
              " '<start>Tom wants to see Mary in his office.<end>',\n",
              " \"<start>We're dating.<end>\",\n",
              " '<start>He is on another line.<end>',\n",
              " '<start>Shopping malls are popular among teenagers.<end>',\n",
              " '<start>Stop screaming in my ears.<end>',\n",
              " '<start>We need to take care of the earth.<end>',\n",
              " '<start>There were flies everywhere.<end>',\n",
              " '<start>A cat has nine lives.<end>',\n",
              " '<start>I need you right now.<end>',\n",
              " '<start>Have you ever eaten at this restaurant?<end>',\n",
              " '<start>He is endowed with many talents.<end>',\n",
              " '<start>I asked Tom if he knew Mary.<end>',\n",
              " '<start>I know Tom is a genius.<end>',\n",
              " '<start>They found him guilty.<end>',\n",
              " '<start>She persuaded him to do it.<end>',\n",
              " '<start>We had to react quickly.<end>',\n",
              " \"<start>I'll do that in the very near future.<end>\",\n",
              " '<start>Choose any dress you like.<end>',\n",
              " '<start>Call home!<end>',\n",
              " '<start>I got him to fix my bicycle.<end>',\n",
              " \"<start>You don't look Japanese.<end>\",\n",
              " '<start>I am just a nobody.<end>',\n",
              " '<start>She volunteered to go to the meeting with him.<end>',\n",
              " '<start>He examined the spare parts one by one.<end>',\n",
              " '<start>He did a cartwheel.<end>',\n",
              " '<start>The shop stays open all day.<end>',\n",
              " \"<start>Let's go outside and sit in the garden.<end>\",\n",
              " \"<start>You aren't allowed to park there.<end>\",\n",
              " '<start>This is a wig.<end>',\n",
              " \"<start>I've been told I'm a pig.<end>\",\n",
              " '<start>What do you need?<end>',\n",
              " '<start>We all laughed.<end>',\n",
              " '<start>They hid in the cellar.<end>',\n",
              " \"<start>I'm comfortable.<end>\",\n",
              " '<start>Air is a mixture of gases that we cannot see.<end>',\n",
              " \"<start>I don't like you very much.<end>\",\n",
              " '<start>What color is this?<end>',\n",
              " '<start>Get your mother.<end>',\n",
              " '<start>He seemed surprised at the news.<end>',\n",
              " '<start>Any house is better than none.<end>',\n",
              " \"<start>I'd like to get a refund.<end>\",\n",
              " '<start>Tom was wondering how he was supposed to get around without a car.<end>',\n",
              " \"<start>I'm quite happy.<end>\",\n",
              " '<start>Tom is already on the bus.<end>',\n",
              " \"<start>He's married.<end>\",\n",
              " \"<start>You're very talented.<end>\",\n",
              " '<start>She stopped singing the song.<end>',\n",
              " '<start>Context is important.<end>',\n",
              " \"<start>I'll have to do it myself.<end>\",\n",
              " '<start>Tom lives in an old building.<end>',\n",
              " '<start>Get outside right now.<end>',\n",
              " \"<start>I'll make all the arrangements.<end>\",\n",
              " '<start>I am older than him.<end>',\n",
              " '<start>To tell the truth, I do not like him.<end>',\n",
              " \"<start>I won't be late.<end>\",\n",
              " '<start>You are such a liar!<end>',\n",
              " '<start>Tom certainly had a point when he said we should allow more time to complete the project.<end>',\n",
              " '<start>He sat next to her.<end>',\n",
              " \"<start>When you've finished reading that book, I'd like to read it.<end>\",\n",
              " '<start>Tom talked to Mary in French.<end>',\n",
              " \"<start>We were amazed at the excellence of the boy's drawings.<end>\",\n",
              " '<start>Are you actually threatening me?<end>',\n",
              " '<start>She has had to stay here.<end>',\n",
              " \"<start>I won't let you jump.<end>\",\n",
              " \"<start>Since this is important, I'd like you to attend to it yourself.<end>\",\n",
              " \"<start>I'm hardworking.<end>\",\n",
              " '<start>It was kind of you to invite us.<end>',\n",
              " '<start>Have you read this book?<end>',\n",
              " '<start>May I be of help in any way?<end>',\n",
              " '<start>I have an urgent message for you.<end>',\n",
              " \"<start>I don't feel like answering questions.<end>\",\n",
              " \"<start>We should consider the problem from a child's perspective.<end>\",\n",
              " '<start>He is very stingy with his money.<end>',\n",
              " \"<start>I don't hate you.<end>\",\n",
              " \"<start>I should've been honest with you.<end>\",\n",
              " '<start>He amazed everyone by passing his driving test.<end>',\n",
              " '<start>She works for French intelligence.<end>',\n",
              " '<start>I had him write it.<end>',\n",
              " '<start>They clung together for warmth.<end>',\n",
              " '<start>Do you know that girl?<end>',\n",
              " \"<start>I would've told you before, but I didn't think you'd understand.<end>\",\n",
              " '<start>I sat down next to him.<end>',\n",
              " '<start>I need more time to prepare.<end>',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fre_train = get_data(list(train['French words/sentences']))"
      ],
      "metadata": {
        "id": "P9CEqu_UeUTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_valid = get_data(list(train['English words/sentences']))\n",
        "fre_valid = get_data(list(train['French words/sentences']))"
      ],
      "metadata": {
        "id": "1S4x-MepeeE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "bJKZ8OaveoUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing and padding input data\n",
        "fre_token = Tokenizer(filters = '', lower = False)\n",
        "fre_token.fit_on_texts(fre_train)\n",
        "fre_tokenized = fre_token.texts_to_sequences(fre_train)\n",
        "fre_padded = pad_sequences(fre_tokenized, padding='post')\n",
        "\n",
        "#tokenizing and padding target data\n",
        "eng_token = Tokenizer(filters = '', lower = False)\n",
        "eng_token.fit_on_texts(eng_train)\n",
        "eng_tokenized = eng_token.texts_to_sequences(eng_train)\n",
        "eng_padded = pad_sequences(fre_tokenized, padding='post')"
      ],
      "metadata": {
        "id": "u73ZHXC-eyUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of unique tokens in input and output languages\n",
        "num_opTokens = len(fre_token.word_index)\n",
        "num_ipTokens = len(eng_token.word_index)\n",
        "\n",
        "#max length of a sentence in both lang\n",
        "max_len_op = fre_padded.shape[1]\n",
        "max_len_ip = eng_padded.shape[1]"
      ],
      "metadata": {
        "id": "fmq0dXHZfpym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fre_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ctSxWfbgX6D",
        "outputId": "35ac2640-5ba7-4e66-8dc1-74b2827c7561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start>Tom a quitté sa voiture.<end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#num of unique tokens in both language\n",
        "print(num_opTokens, num_ipTokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FovjwpFhgtaa",
        "outputId": "651281b8-049f-4c45-a9eb-eb5a3c72c064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40287 23885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max length of sentence in both language\n",
        "print(max_len_op, max_len_ip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJoYXIWVgaN_",
        "outputId": "2b861881-dcbd-4609-f356-d75b274c055c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ktext"
      ],
      "metadata": {
        "id": "5I1kBu9pg2aB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176c4f8d-43c6-401e-d62a-1d72b5cbd7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ktext in /usr/local/lib/python3.10/dist-packages (0.34)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ktext) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ktext) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ktext) (1.16.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from ktext) (1.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ktext) (6.0)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from ktext) (0.3.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from ktext) (1.0.5)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.10/dist-packages (from ktext) (0.4.8)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from ktext) (2022.12.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from ktext) (9.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from ktext) (9.1.0)\n",
            "Requirement already satisfied: textacy<=0.6.2 in /usr/local/lib/python3.10/dist-packages (from ktext) (0.6.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from ktext) (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from ktext) (2.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.21.0->ktext) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.21.0->ktext) (2022.7.1)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (5.3.0)\n",
            "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (0.12.1)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (4.4.3)\n",
            "Requirement already satisfied: ijson>=2.3 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (3.2.0.post0)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (3.1)\n",
            "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (1.0.0)\n",
            "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (0.14.0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (0.21.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (1.2.2)\n",
            "Requirement already satisfied: spacy>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (3.5.2)\n",
            "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (4.65.0)\n",
            "Requirement already satisfied: unidecode>=0.04.19 in /usr/local/lib/python3.10/dist-packages (from textacy<=0.6.2->ktext) (1.3.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (8.1.3)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (23.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (1.4.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask->ktext) (0.12.0)\n",
            "Requirement already satisfied: ppft>=1.7.6.6 in /usr/local/lib/python3.10/dist-packages (from pathos->ktext) (1.7.6.6)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from pathos->ktext) (0.3.6)\n",
            "Requirement already satisfied: pox>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from pathos->ktext) (0.3.2)\n",
            "Requirement already satisfied: multiprocess>=0.70.14 in /usr/local/lib/python3.10/dist-packages (from pathos->ktext) (0.70.14)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (0.4.8)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->ktext) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->ktext) (0.40.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (0.2.6)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->ktext) (0.1.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask->ktext) (1.0.0)\n",
            "Requirement already satisfied: Levenshtein==0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-levenshtein>=0.12.0->textacy<=0.6.2->ktext) (0.21.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.21.0->python-levenshtein>=0.12.0->textacy<=0.6.2->ktext) (3.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.0->textacy<=0.6.2->ktext) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.0->textacy<=0.6.2->ktext) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (3.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->ktext) (2.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->ktext) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->ktext) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->ktext) (1.3.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.0->textacy<=0.6.2->ktext) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.0->textacy<=0.6.2->ktext) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->ktext) (2.1.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (0.5.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->ktext) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->ktext) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqBf-0VXiJsd",
        "outputId": "8aa5a9ed-5674-4a3a-bc71-f5da59237006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 2341, in parseImpl\n",
            "    raise ParseException(instring, loc, self.errmsg, self)\n",
            "pip._vendor.pyparsing.exceptions.ParseException: Expected 'python_implementation', found \"'\"  (at char 32), (line:1, col:33)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1734, in isEnabledFor\n",
            "    _acquireLock()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 226, in _acquireLock\n",
            "    _lock.acquire()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brcE9dSXnmYt",
        "outputId": "4a3711df-2d82-48ca-8e6e-ec552bcb0c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "CmknI1pQ4lNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ktext.preprocess import processor"
      ],
      "metadata": {
        "id": "kqcJGpGahHBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pp = processor(keep_n=25407, padding_maxlen=55)\n",
        "eng_train_vec = eng_pp.fit_transform(eng_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmwujlAJg9A1",
        "outputId": "11b96888-5447-411a-cf5b-ec2d2b67267a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 14 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 112,396 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fre_pp = processor(keep_n=41000, padding_maxlen=55, append_indicators=True, padding='post')\n",
        "fre_train_vec = fre_pp.fit_transform(fre_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDvd_7k4piDA",
        "outputId": "bae7e44f-3fb5-4cdc-aedb-108d235aef0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 23 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 112,396 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dill as dpickle\n",
        "import numpy as np\n",
        "\n",
        "with open('eng_pp.dpkl', 'wb') as f:\n",
        "  dpickle.dump(eng_pp, f)\n",
        "\n",
        "with open('fre_pp.dpkl', 'wb') as f:\n",
        "  dpickle.dump(fre_pp, f)\n",
        "\n",
        "np.save('eng_train_vecs.npy', eng_train_vec)\n",
        "np.save('fre_train_vecs.npy', fre_train_vec)"
      ],
      "metadata": {
        "id": "G_MQwSAnp-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_encoder_input(encoder_np_vecs='eng_train_vecs.npy'):\n",
        "  vectorizedBody = np.load(encoder_np_vecs)\n",
        "  encoder_input_data = vectorizedBody\n",
        "  doc_length  = encoder_input_data.shape[1]\n",
        "\n",
        "  print(f'Shape of encoder input: {encoder_input_data.shape}')\n",
        "  return encoder_input_data, doc_length\n",
        "\n",
        "def load_decoder_input(decoder_np_vecs='fre_train_vecs.npy'):\n",
        "  vectorizedTitle = np.load(decoder_np_vecs)\n",
        "  decoder_input_data = vectorizedTitle[:, :-1]\n",
        "  decoder_target_data = vectorizedTitle[:, :-1]\n",
        "\n",
        "  print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
        "  print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
        "  return decoder_input_data, decoder_target_data"
      ],
      "metadata": {
        "id": "LfX-8849qgEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_processor(fname='eng_pp.dpkl'):\n",
        "  with open(fname, 'rb') as f:\n",
        "    pp = dpickle.load(f)\n",
        "\n",
        "    numTokens  = max(pp.id2token.keys()) + 1\n",
        "    print(f'Size of vocabulary for {fname} : {numTokens:,}')\n",
        "    return numTokens, pp"
      ],
      "metadata": {
        "id": "dJ13auJIsHfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens, eng_pp = load_text_processor('eng_pp.dpkl')\n",
        "num_decoder_tokens, fre_pp = load_text_processor('fre_pp.dpkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL4qSXr2si9u",
        "outputId": "73368695-6fca-4d7a-e540-1abdc0cd8c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary for eng_pp.dpkl : 12,089\n",
            "Size of vocabulary for fre_pp.dpkl : 19,409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data, doc_length = load_encoder_input('eng_train_vecs.npy')\n",
        "decoder_input_data, decoder_target_data = load_decoder_input('fre_train_vecs.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wga9OJ8Rstto",
        "outputId": "eca36aae-9e06-43b6-9626-c09f188a7e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of encoder input: (112396, 55)\n",
            "Shape of decoder input: (112396, 54)\n",
            "Shape of decoder target: (112396, 54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\noriginal string:\\n', eng_train[0], '\\n')\n",
        "print('\\nAfter preprocessing:\\n', encoder_input_data[0], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5w6VOOAtOuM",
        "outputId": "e37755f3-fcec-4951-b041-a949047b499b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "original string:\n",
            " <start>Tom left in his car.<end> \n",
            "\n",
            "\n",
            "After preprocessing:\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   2  12 180  19  44 128\n",
            "   3] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\noriginal string:\\n', fre_train[0], '\\n')\n",
        "print('After preprocessing:\\n', decoder_input_data[0], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pPrdfLUuSAS",
        "outputId": "f2ea08f8-dff6-4880-bd46-875e626d338a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "original string:\n",
            " <start>Tom a quitté sa voiture.<end> \n",
            "\n",
            "After preprocessing:\n",
            " [  3   4  21   7 831 102 138   2   5   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "RHzrU-rWg2iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 300\n",
        "\n",
        "encoder_inputs = Input(shape=(doc_length,), name='Encoder_Input')\n",
        "\n",
        "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word_Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
        "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "decoder_gru  = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "\n",
        "\n",
        "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "5O1zxldnvsFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_Model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJN3MqPbyycq",
        "outputId": "dabee3c1-73a4-4879-e44b-41d744b0a47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Word-Embedding (Embedd  (None, None, 300)   5822700     ['Decoder-Input[0][0]']          \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " Encoder_Input (InputLayer)     [(None, 55)]         0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-1 (BatchNorm  (None, None, 300)   1200        ['Decoder-Word-Embedding[0][0]'] \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-Model (Functional)     (None, 300)          4169700     ['Encoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-GRU (GRU)              [(None, None, 300),  541800      ['Decoder-Batchnorm-1[0][0]',    \n",
            "                                 (None, 300)]                     'Encoder-Model[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-2 (BatchNorm  (None, None, 300)   1200        ['Decoder-GRU[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Final-Output-Dense (Dense)     (None, None, 19409)  5842109     ['Decoder-Batchnorm-2[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,378,709\n",
            "Trainable params: 16,376,909\n",
            "Non-trainable params: 1,800\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "script_name_base = 'MT_seq2seq'\n",
        "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
        "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}--val{{val_loss:.5f}}.hdf5'.format(script_name_base), \n",
        "                                   save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "\n",
        "batch_size = 1200\n",
        "epochs = 10\n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, axis=-1),\n",
        "                            batch_size=batch_size, epochs=epochs, validation_split=0.12,\n",
        "                            callbacks=[csv_logger, model_checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jg2R7E0y7yj",
        "outputId": "17a7ca96-4c8b-4471-9e47-7310a5b52ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "83/83 [==============================] - 127s 1s/step - loss: 2.6370 - val_loss: 8.2670\n",
            "Epoch 2/10\n",
            "83/83 [==============================] - 123s 1s/step - loss: 0.0825 - val_loss: 8.4812\n",
            "Epoch 3/10\n",
            "83/83 [==============================] - 120s 1s/step - loss: 0.0280 - val_loss: 7.7384\n",
            "Epoch 4/10\n",
            "83/83 [==============================] - 119s 1s/step - loss: 0.0119 - val_loss: 6.3780\n",
            "Epoch 5/10\n",
            "83/83 [==============================] - 124s 1s/step - loss: 0.0047 - val_loss: 4.3411\n",
            "Epoch 6/10\n",
            "83/83 [==============================] - 120s 1s/step - loss: 0.0026 - val_loss: 1.8466\n",
            "Epoch 7/10\n",
            "83/83 [==============================] - 119s 1s/step - loss: 0.0019 - val_loss: 0.3769\n",
            "Epoch 8/10\n",
            "83/83 [==============================] - 119s 1s/step - loss: 0.0015 - val_loss: 0.0546\n",
            "Epoch 9/10\n",
            "83/83 [==============================] - 119s 1s/step - loss: 0.0012 - val_loss: 0.0213\n",
            "Epoch 10/10\n",
            "83/83 [==============================] - 119s 1s/step - loss: 0.0010 - val_loss: 0.0169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del seq2seq_Model"
      ],
      "metadata": {
        "id": "YGZCy7fv1asB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "-kEMcI7xGAZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_Model.load_weights('MT_seq2seq.epoch10--val0.01690.hdf5')"
      ],
      "metadata": {
        "id": "iOYyl-IUGTiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_encoder_model(model):\n",
        "  encoder_model = model.get_layer('Encoder-Model')\n",
        "  return encoder_model\n",
        "\n",
        "def extract_decoder_model(model):\n",
        "  latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n",
        "\n",
        "  decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "  dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "  dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "  gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "\n",
        "  gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "\n",
        "  dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "  dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "  decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
        "                        [dense_out, gru_state_out])\n",
        "  return decoder_model"
      ],
      "metadata": {
        "id": "YexPW5lHGhgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}